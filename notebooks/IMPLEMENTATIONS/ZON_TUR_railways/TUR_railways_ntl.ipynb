{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish railways - pop and ntl\n",
    "\n",
    "Compare population and nighttime lights around existing, under-construction, and proposed railways in Turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# GOSTrocks is pipable, but this ensures I am working on my local development version\n",
    "sys.path.insert(0, \"../../../../GOSTrocks/src\")\n",
    "import GOSTrocks.ntlMisc as ntlMisc\n",
    "import GOSTrocks.rasterMisc as rMisc\n",
    "from GOSTrocks.misc import tPrint\n",
    "\n",
    "from space2stats_client import Space2StatsClient\n",
    "from shapely import from_geojson\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "client = Space2StatsClient(verify_ssl=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "base_folder = r\"C:\\WBG\\Work\\Projects\\TUR_Railways\"\n",
    "railway_folder = os.path.join(base_folder, \"Data\", \"Source\")\n",
    "results_folder = os.path.join(base_folder, \"Data\", \"Results\")\n",
    "\n",
    "projected_railways = os.path.join(railway_folder, \"Projected Railways\", \"doc.kml\")\n",
    "under_construction_railways = os.path.join(railway_folder, \"Railways Under Construction\", \"doc.kml\")\n",
    "existing_railways = os.path.join(railway_folder, \"existing\", \"turrail.shp\")\n",
    "\n",
    "pop_file = os.path.join(base_folder, \"Data\", \"tur_ppp_2020_UNadj_constrained.tif\")\n",
    "admin_bounds = os.path.join(base_folder, \"Data\", \"TUR_ADM_1.gpkg\")\n",
    "metro_def_file = os.path.join(base_folder, \"Data\", \"TUR_ADM1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get admin bouncdaries and metadata\n",
    "ISO3 = \"TUR\" # Turkey\n",
    "ADM = \"ADM2\" # Level 2 administrative boundaries\n",
    "m_crs = 5636\n",
    "adm_boundaries = client.fetch_admin_boundaries(ISO3, ADM)\n",
    "adm1_boundaries = gpd.read_file(admin_bounds)\n",
    "metro_def = pd.read_csv(metro_def_file)\n",
    "adm1_boundaries = pd.merge(adm1_boundaries, metro_def.loc[:,[\"ADM1CD_c\", \"Metropolitan\"]], on=\"ADM1CD_c\")\n",
    "national_bounds = client.fetch_admin_boundaries(ISO3, \"ADM0\")\n",
    "\n",
    "# List all S2S topics\n",
    "topics = client.get_topics()\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the nighttime lights values for the AOI\n",
    "properties = client.get_properties(\"nighttime_lights\")\n",
    "sel_fields = list(properties['name'].values[:-1])\n",
    "df = client.get_summary(\n",
    "    gdf=adm1_boundaries,                     # Area of Interest\n",
    "    spatial_join_method=\"centroid\",         # Spatial join method (between h3 cells and each feature)\n",
    "    fields=sel_fields,                      # Fields from Space2Stats to query\n",
    "    geometry=\"polygon\"                      # Whether to return the geometry of the hexagons\n",
    ")\n",
    "\n",
    "df[\"geometry\"] = df[\"geometry\"].apply(lambda geom: from_geojson(geom))\n",
    "\n",
    "# Convert dataframe to GeoDataFrame\n",
    "tur_s2s = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "tur_s2s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h3ronpy.pandas.vector import geodataframe_to_cells, cells_dataframe_to_geodataframe\n",
    "from h3ronpy import ContainmentMode\n",
    "\n",
    "def get_bounds(in_shp, gID, h3_lvl=6):\n",
    "    \"\"\" Generate a geodataframe for the supplied in_shp with the H3 cells and % overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_shp : geopandas.GeoDataFrame\n",
    "        The input shapely polygon as a geopandas dataframe\n",
    "    gID : string\n",
    "        The column name to use for the ID in the output\n",
    "    h3_lvl : int\n",
    "        The H3 level to use for the hexagons, default is 6\n",
    "    \"\"\"\n",
    "    # extract the H3 cells\n",
    "    cols_to_keep = [gID, 'cell', 'overlap']\n",
    "    cell_ax = cells_dataframe_to_geodataframe(geodataframe_to_cells(in_shp, 6, ContainmentMode.IntersectsBoundary))\n",
    "    cell_ax['cell'] = cell_ax['cell'].apply(lambda x: hex(x)[2:])    \n",
    "    # Identify contained and overlapping hexes with the admin bounds\n",
    "    contained_h3 = cell_ax.sjoin(in_shp, predicate='within')\n",
    "    missed_h3 = cell_ax[~cell_ax['cell'].isin(contained_h3['cell'])]\n",
    "    # calculate h3x overlap with feature\n",
    "    shp_area = in_shp.union_all()\n",
    "    cell_ax['overlap'] = 0.0\n",
    "    cell_ax.loc[contained_h3.index, 'overlap'] = 1.0\n",
    "    cell_ax.loc[missed_h3.index, 'overlap'] = cell_ax.loc[missed_h3.index,'geometry'].apply(lambda x: x.intersection(shp_area).area/x.area)\\\n",
    "    \n",
    "    return cell_ax.loc[:, cols_to_keep].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total nighttime lights for each administrative area, based on overlap with the administrative boundaries\n",
    "for lbl, curD in tur_s2s.groupby(\"ADM1CD_c\"):\n",
    "    cur_admin_boundary = adm1_boundaries[adm1_boundaries[\"ADM1CD_c\"] == lbl]\n",
    "    hex_overlap = get_bounds(cur_admin_boundary.union_all(), \"ADM1CD_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The population values are not correct in S2S, update with specific input pop raster from the project team\n",
    "res = rMisc.zonalStats(tur_s2s, pop_file, minVal=0, return_df=True)\n",
    "tur_s2s['sum_pop_2020'] = res['SUM']\n",
    "sel_fields = sel_fields + ['sum_pop_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "tur_s2s.plot(ax=ax, column=\"sum_viirs_ntl_2012\", \n",
    "         legend=True, cmap=\"Reds\", alpha=0.75, \n",
    "         scheme=\"naturalbreaks\", k=5, \n",
    "         legend_kwds=dict(title='NTL 2012', fmt=\"{:,.0f}\"),\n",
    "         linewidth=0)\n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldPhysical, verify=False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize nighttime lights and population within 15km of various railways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KML/KMZ files from the source are garbage. In order to use the KML files, we need to read them and extract the layers oddly\n",
    "from lxml import etree\n",
    "\n",
    "def list_kml_layers(filename, method=1):\n",
    "    tree = etree.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    namespaces = {'kml': 'http://www.opengis.net/kml/2.2'}\n",
    "\n",
    "    if method == 1:\n",
    "        \n",
    "        layers = root.findall(\".//{kml}Document\", namespaces)\n",
    "        layer_names = []\n",
    "        for layer in layers:\n",
    "            name_element = layer.find('{kml}name', namespaces)\n",
    "            if name_element is not None:\n",
    "                layer_names.append(name_element.text)\n",
    "            else:\n",
    "                layer_names.append(\"Unnamed Layer\")\n",
    "        return layer_names\n",
    "    else:\n",
    "        layers = []\n",
    "        for element in root.iter():\n",
    "            if element.tag.endswith('Document') or element.tag.endswith('Folder'):\n",
    "                name_element = element.find('kml:name', namespaces)\n",
    "                layer_name = name_element.text if name_element is not None else \"Unnamed Layer\"\n",
    "                layers.append(layer_name)\n",
    "        return layers\n",
    "\n",
    "def gpd_read_all_layers(filename):\n",
    "    layers = list_kml_layers(filename, method=1)\n",
    "    good_layers = []\n",
    "    for cur_layer in layers:\n",
    "        try:\n",
    "            curD = gpd.read_file(filename, driver=\"KML\", layer=cur_layer)\n",
    "            print(f\"Processing layer: {cur_layer} - {curD.union_all().geom_type}\")\n",
    "            if curD.union_all().geom_type in [\"LineString\", \"MultiLineString\"]:\n",
    "                curD['Label'] = cur_layer\n",
    "                good_layers.append(curD)\n",
    "        except:\n",
    "            print(f\"Layer {cur_layer} not found or could not be read.\")\n",
    "    return pd.concat(good_layers, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_projected_file = os.path.join(railway_folder, \"projected_railways.gpkg\")\n",
    "updated_construction_file = os.path.join(railway_folder, \"under_construction_railways.gpkg\")\n",
    "if not os.path.exists(updated_projected_file):\n",
    "    projected_rail_gpd = gpd_read_all_layers(projected_railways)\n",
    "    projected_rail_gpd.to_file(updated_projected_file, driver=\"GPKG\")\n",
    "else:\n",
    "    projected_rail_gpd = gpd.read_file(updated_projected_file)\n",
    "if not os.path.exists(updated_construction_file):\n",
    "    under_construction_rail_gpd = gpd.read_file(under_construction_railways, driver=\"KML\")    \n",
    "else:\n",
    "    under_construction_rail_gpd = gpd.read_file(updated_construction_file)\n",
    "\n",
    "existing_rail_gpd = gpd.read_file(existing_railways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert projected and under construction to single row GeoDataFrames\n",
    "projected_rail_gpd = projected_rail_gpd.dissolve().reset_index(drop=True)\n",
    "under_construction_rail_gpd = under_construction_rail_gpd.dissolve().explode().reset_index(drop=True)\n",
    "under_construction_rail_gpd.to_file(updated_construction_file, driver=\"GPKG\")\n",
    "\n",
    "# The under_construction railways are present in the projected railways file, so we need to remove them\n",
    "projected_rail_gpd['geometry'].iloc[0] = projected_rail_gpd['geometry'].iloc[0].difference(under_construction_rail_gpd['geometry'].iloc[0])\n",
    "\n",
    "# Explode the geometries to have one row per segment\n",
    "projected_rail_gpd = projected_rail_gpd.dissolve().explode().reset_index(drop=True)\n",
    "projected_rail_gpd = projected_rail_gpd.to_crs(epsg=m_crs)\n",
    "projected_rail_gpd['length'] = projected_rail_gpd['geometry'].length\n",
    "projected_rail_gpd.sort_values(by='length')\n",
    "projected_rail_gpd = projected_rail_gpd.loc[projected_rail_gpd['length'] > 10]\n",
    "projected_rail_gpd.loc[projected_rail_gpd['length'] > 10].to_file(os.path.join(railway_folder, \"projected_railways_long.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2s_sums(in_shape, s2s, buffer_dist=15000):\n",
    "    # read in railways and buffer\n",
    "    in_shape_buffer = in_shape.to_crs(m_crs)  \n",
    "    in_shape_buffer[\"geometry\"] = in_shape_buffer.buffer(buffer_dist)\n",
    "    all_shape = in_shape_buffer.union_all() \n",
    "\n",
    "    # Identify S2S hexagons that intersect with the buffered railways\n",
    "    s2s_cols = s2s.columns.tolist()\n",
    "    s2s = gpd.sjoin(s2s, in_shape_buffer, how=\"inner\", predicate=\"intersects\")\n",
    "    s2s = s2s.drop_duplicates(subset=[\"hex_id\"])\n",
    "    s2s = s2s.loc[:, s2s_cols]  # Keep only the original S2S columns\n",
    "    \n",
    "    # determine S2S overlap with buffered railways\n",
    "    s2s['overlap'] = s2s['geometry'].apply(lambda x: x.intersection(all_shape).area/x.area)\n",
    "    combo_h3 = s2s.sjoin(in_shape_buffer, how=\"inner\", predicate=\"intersects\")\n",
    "    combo_h3 = combo_h3.drop_duplicates(subset=[\"hex_id\"])\n",
    "    \n",
    "    #Calculate sums based on overlap\n",
    "    all_results = {}\n",
    "    for col in sel_fields:\n",
    "        cur_results = (combo_h3[col] * combo_h3['overlap']).sum()\n",
    "        all_results[col] = cur_results\n",
    "    return all_results\n",
    "\n",
    "railway_results_file = os.path.join(results_folder, \"railway_s2s_summary.csv\")\n",
    "all_rails = pd.concat([existing_rail_gpd,under_construction_rail_gpd], ignore_index=True)\n",
    "if not os.path.exists(railway_results_file):\n",
    "    if tur_s2s.crs != m_crs:\n",
    "        tur_s2s = tur_s2s.to_crs(m_crs)\n",
    "    tPrint(\"Processing railways and S2S\")\n",
    "    existing_res = get_s2s_sums(existing_rail_gpd, tur_s2s, buffer_dist=15000)    \n",
    "    tPrint(\"Completed existing railways\")\n",
    "    #projected_res = get_s2s_sums(projected_rail_gpd.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    tPrint(\"Completed projected railways\")\n",
    "    under_construction_res = get_s2s_sums(under_construction_rail_gpd.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    tPrint(\"Completed under construction railways\")\n",
    "    all_res = get_s2s_sums(all_rails.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    pd.DataFrame({\n",
    "        \"All Railways\": all_res,\n",
    "        \"Existing Railways\": existing_res,\n",
    "     #   \"Projected Railways\": projected_res,\n",
    "        \"Under Construction Railways\": under_construction_res\n",
    "    }).T.to_csv(railway_results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize effects on muncipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_boundaries = adm1_boundaries.to_crs(m_crs)\n",
    "# run summaries on tur_s2s\n",
    "all_res = {}\n",
    "for lbl, df in tur_s2s.groupby(\"ADM1CD_c\"):\n",
    "    adm_shape = adm1_boundaries.loc[adm1_boundaries[\"ADM1CD_c\"] == lbl, \"geometry\"].values[0]\n",
    "    df['overlap'] = df['geometry'].apply(lambda x: x.intersection(adm_shape).area/x.area)\n",
    "    results = {}\n",
    "    for col in sel_fields + ['sum_pop_2020']:\n",
    "        results[col] = (df[col] * df['overlap']).sum()\n",
    "    all_res[lbl] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_summaries = pd.merge(adm1_boundaries, pd.DataFrame(all_res).T, left_on=\"ADM1CD_c\", right_index=True)\n",
    "adm_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the metropolitan areas that intersect railways\n",
    "metro_adm = adm_summaries.loc[adm_summaries['Metropolitan'] == 1]\n",
    "\n",
    "# project all layers to m_crs\n",
    "projected_rail_gpd = projected_rail_gpd.to_crs(epsg=m_crs)\n",
    "under_construction_rail_gpd = under_construction_rail_gpd.to_crs(epsg=m_crs)\n",
    "existing_rail_gpd = existing_rail_gpd.to_crs(epsg=m_crs)\n",
    "metro_adm = metro_adm.to_crs(epsg=m_crs)\n",
    "metro_adm['geometry'] = metro_adm.buffer(15000)  # Buffer the metropolitan areas\n",
    "\n",
    "# Check which metropolitan areas intersect projected railways\n",
    "metro_intersections_pr = metro_adm.sjoin(projected_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_pr = metro_intersections_pr.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "# Check with which metropolitan areas intersect under construction railways\n",
    "metro_intersections_uc = metro_adm.sjoin(under_construction_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_uc = metro_intersections_uc.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "# Check which metropolitan areas intersect existing railways\n",
    "metro_intersections_ex = metro_adm.sjoin(existing_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_ex = metro_intersections_ex.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "\n",
    "for cur_intersections, label in zip(\n",
    "        [metro_intersections_pr, metro_intersections_uc, metro_intersections_ex],\n",
    "        [\"Projected Railways\", \"Under Construction Railways\", \"Existing Railways\"]\n",
    "    ):\n",
    "    print(f\"\\n{label}, {cur_intersections.shape[0]}, {cur_intersections['sum_pop_2020'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ntl layers\n",
    "ntl_files = ntlMisc.generate_annual_composites(adm_boundaries, out_folder=os.path.join(base_folder, \"Data\", \"NTL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_s2s.to_file(os.path.join(results_folder, \"tur_s2s.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_boundaries.to_file(admin_bounds.replace(\".gpkg\", \"_metro.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_ingest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
