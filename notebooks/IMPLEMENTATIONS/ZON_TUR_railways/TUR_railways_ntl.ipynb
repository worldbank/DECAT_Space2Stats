{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish railways - pop and ntl\n",
    "\n",
    "Compare population and nighttime lights around existing, under-construction, and proposed railways in Turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import requests\n",
    "import fiona\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "sys.path.insert(0, \"C:/WBG/Work/Code/GOSTrocks/src\")\n",
    "import GOSTrocks.ntlMisc as ntlMisc\n",
    "import GOSTrocks.rasterMisc as rMisc\n",
    "\n",
    "from GOSTrocks.misc import tPrint\n",
    "from space2stats_client import Space2StatsClient\n",
    "from shapely import from_geojson\n",
    "\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "client = Space2StatsClient(verify_ssl=False)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = r\"C:\\WBG\\Work\\Projects\\TUR_Railways\"\n",
    "railway_folder = os.path.join(base_folder, \"Data\", \"Source\")\n",
    "results_folder = os.path.join(base_folder, \"Data\", \"Results\")\n",
    "\n",
    "projected_railways = os.path.join(railway_folder, \"Projected Railways\", \"doc.kml\")\n",
    "under_construction_railways = os.path.join(railway_folder, \"Railways Under Construction\", \"doc.kml\")\n",
    "existing_railways = os.path.join(railway_folder, \"existing\", \"turrail.shp\")\n",
    "\n",
    "pop_file = os.path.join(base_folder, \"Data\", \"tur_ppp_2020_UNadj_constrained.tif\")\n",
    "admin_bounds = os.path.join(base_folder, \"Data\", \"TUR_ADM_1.gpkg\")\n",
    "metro_def_file = os.path.join(base_folder, \"Data\", \"TUR_ADM1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Item ID</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>source_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>space2stats_population_2020</th>\n",
       "      <td>Population</td>\n",
       "      <td>Gridded population disaggregated by gender.</td>\n",
       "      <td>WorldPop gridded population, 2020, Unconstrain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flood_exposure_15cm_1in100</th>\n",
       "      <td>Population Exposed to Floods</td>\n",
       "      <td>Population where flood depth is greater than 1...</td>\n",
       "      <td>Fathom 3.0 High Resolution Global Flood Maps I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urbanization_ghssmod</th>\n",
       "      <td>Urbanization by population and by area</td>\n",
       "      <td>Urbanization is analyzed using the GHS-SMOD da...</td>\n",
       "      <td>Global Human Settlement Layer (https://human-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nighttime_lights</th>\n",
       "      <td>Nighttime Lights</td>\n",
       "      <td>Sum of luminosity values measured by monthly c...</td>\n",
       "      <td>World Bank - Light Every Night, https://regist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>Standardized Precipitation Index (SPI)</td>\n",
       "      <td>Index for a given timescale measuring drought ...</td>\n",
       "      <td>CHIRPS3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>builtarea_ghsl</th>\n",
       "      <td>Built area</td>\n",
       "      <td>Built area (in m2) in 5-year epochs. Source da...</td>\n",
       "      <td>https://human-settlement.emergency.copernicus....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Item ID                                                        name  \\\n",
       "space2stats_population_2020                              Population   \n",
       "flood_exposure_15cm_1in100             Population Exposed to Floods   \n",
       "urbanization_ghssmod         Urbanization by population and by area   \n",
       "nighttime_lights                                   Nighttime Lights   \n",
       "climate                      Standardized Precipitation Index (SPI)   \n",
       "builtarea_ghsl                                           Built area   \n",
       "\n",
       "Item ID                                                            description  \\\n",
       "space2stats_population_2020        Gridded population disaggregated by gender.   \n",
       "flood_exposure_15cm_1in100   Population where flood depth is greater than 1...   \n",
       "urbanization_ghssmod         Urbanization is analyzed using the GHS-SMOD da...   \n",
       "nighttime_lights             Sum of luminosity values measured by monthly c...   \n",
       "climate                      Index for a given timescale measuring drought ...   \n",
       "builtarea_ghsl               Built area (in m2) in 5-year epochs. Source da...   \n",
       "\n",
       "Item ID                                                            source_data  \n",
       "space2stats_population_2020  WorldPop gridded population, 2020, Unconstrain...  \n",
       "flood_exposure_15cm_1in100   Fathom 3.0 High Resolution Global Flood Maps I...  \n",
       "urbanization_ghssmod         Global Human Settlement Layer (https://human-s...  \n",
       "nighttime_lights             World Bank - Light Every Night, https://regist...  \n",
       "climate                                                                CHIRPS3  \n",
       "builtarea_ghsl               https://human-settlement.emergency.copernicus....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ISO3 = \"TUR\" # Turkey\n",
    "ADM = \"ADM2\" # Level 2 administrative boundaries\n",
    "m_crs = 5636\n",
    "adm_boundaries = client.fetch_admin_boundaries(ISO3, ADM)\n",
    "adm1_boundaries = gpd.read_file(admin_bounds)\n",
    "metro_def = pd.read_csv(metro_def_file)\n",
    "adm1_boundaries = pd.merge(adm1_boundaries, metro_def.loc[:,[\"ADM1CD_c\", \"Metropolitan\"]], on=\"ADM1CD_c\")\n",
    "national_bounds = client.fetch_admin_boundaries(ISO3, \"ADM0\")\n",
    "\n",
    "# List all S2S topics\n",
    "topics = client.get_topics()\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for boundary 1 of 80...\n",
      "Fetching data for boundary 2 of 80...\n",
      "Fetching data for boundary 3 of 80...\n",
      "Fetching data for boundary 4 of 80...\n",
      "Fetching data for boundary 5 of 80...\n",
      "Fetching data for boundary 6 of 80...\n",
      "Fetching data for boundary 7 of 80...\n",
      "Fetching data for boundary 8 of 80...\n",
      "Fetching data for boundary 9 of 80...\n",
      "Fetching data for boundary 10 of 80...\n",
      "Fetching data for boundary 11 of 80...\n",
      "Fetching data for boundary 12 of 80...\n",
      "Fetching data for boundary 13 of 80...\n",
      "Fetching data for boundary 14 of 80...\n",
      "Fetching data for boundary 15 of 80...\n",
      "Fetching data for boundary 16 of 80...\n",
      "Fetching data for boundary 17 of 80...\n",
      "Fetching data for boundary 18 of 80...\n",
      "Fetching data for boundary 19 of 80...\n",
      "Fetching data for boundary 20 of 80...\n",
      "Fetching data for boundary 21 of 80...\n",
      "Fetching data for boundary 22 of 80...\n",
      "Fetching data for boundary 23 of 80...\n",
      "Fetching data for boundary 24 of 80...\n",
      "Fetching data for boundary 25 of 80...\n",
      "Fetching data for boundary 26 of 80...\n",
      "Fetching data for boundary 27 of 80...\n",
      "Fetching data for boundary 28 of 80...\n",
      "Fetching data for boundary 29 of 80...\n",
      "Fetching data for boundary 30 of 80...\n",
      "Fetching data for boundary 31 of 80...\n",
      "Fetching data for boundary 32 of 80...\n",
      "Fetching data for boundary 33 of 80...\n",
      "Fetching data for boundary 34 of 80...\n",
      "Fetching data for boundary 35 of 80...\n",
      "Fetching data for boundary 36 of 80...\n",
      "Fetching data for boundary 37 of 80...\n",
      "Fetching data for boundary 38 of 80...\n",
      "Fetching data for boundary 39 of 80...\n",
      "Fetching data for boundary 40 of 80...\n",
      "Fetching data for boundary 41 of 80...\n",
      "Fetching data for boundary 42 of 80...\n",
      "Fetching data for boundary 43 of 80...\n",
      "Fetching data for boundary 44 of 80...\n",
      "Fetching data for boundary 45 of 80...\n",
      "Fetching data for boundary 46 of 80...\n",
      "Fetching data for boundary 47 of 80...\n",
      "Fetching data for boundary 48 of 80...\n",
      "Fetching data for boundary 49 of 80...\n",
      "Fetching data for boundary 50 of 80...\n",
      "Fetching data for boundary 51 of 80...\n",
      "Fetching data for boundary 52 of 80...\n",
      "Fetching data for boundary 53 of 80...\n",
      "Fetching data for boundary 54 of 80...\n",
      "Fetching data for boundary 55 of 80...\n",
      "Fetching data for boundary 56 of 80...\n",
      "Fetching data for boundary 57 of 80...\n",
      "Fetching data for boundary 58 of 80...\n",
      "Fetching data for boundary 59 of 80...\n",
      "Fetching data for boundary 60 of 80...\n",
      "Fetching data for boundary 61 of 80...\n",
      "Fetching data for boundary 62 of 80...\n",
      "Fetching data for boundary 63 of 80...\n",
      "Fetching data for boundary 64 of 80...\n",
      "Fetching data for boundary 65 of 80...\n",
      "Fetching data for boundary 66 of 80...\n",
      "Fetching data for boundary 67 of 80...\n",
      "Fetching data for boundary 68 of 80...\n",
      "Fetching data for boundary 69 of 80...\n",
      "Fetching data for boundary 70 of 80...\n",
      "Fetching data for boundary 71 of 80...\n",
      "Fetching data for boundary 72 of 80...\n",
      "Fetching data for boundary 73 of 80...\n",
      "Fetching data for boundary 74 of 80...\n",
      "Fetching data for boundary 75 of 80...\n",
      "Fetching data for boundary 76 of 80...\n",
      "Fetching data for boundary 77 of 80...\n",
      "Fetching data for boundary 78 of 80...\n",
      "Fetching data for boundary 79 of 80...\n",
      "Fetching data for boundary 80 of 80...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO_A3</th>\n",
       "      <th>ISO_A2</th>\n",
       "      <th>WB_A3</th>\n",
       "      <th>HASC_0</th>\n",
       "      <th>HASC_1</th>\n",
       "      <th>GAUL_0</th>\n",
       "      <th>GAUL_1</th>\n",
       "      <th>WB_REGION</th>\n",
       "      <th>WB_STATUS</th>\n",
       "      <th>SOVEREIGN</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_viirs_ntl_2015</th>\n",
       "      <th>sum_viirs_ntl_2016</th>\n",
       "      <th>sum_viirs_ntl_2017</th>\n",
       "      <th>sum_viirs_ntl_2018</th>\n",
       "      <th>sum_viirs_ntl_2019</th>\n",
       "      <th>sum_viirs_ntl_2020</th>\n",
       "      <th>sum_viirs_ntl_2021</th>\n",
       "      <th>sum_viirs_ntl_2022</th>\n",
       "      <th>sum_viirs_ntl_2023</th>\n",
       "      <th>sum_viirs_ntl_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TR.AV</td>\n",
       "      <td>249</td>\n",
       "      <td>3026</td>\n",
       "      <td>ECA</td>\n",
       "      <td>Member State</td>\n",
       "      <td>TUR</td>\n",
       "      <td>...</td>\n",
       "      <td>179.200012</td>\n",
       "      <td>297.839996</td>\n",
       "      <td>257.440002</td>\n",
       "      <td>183.550018</td>\n",
       "      <td>241.480011</td>\n",
       "      <td>285.760010</td>\n",
       "      <td>331.869995</td>\n",
       "      <td>288.160034</td>\n",
       "      <td>346.290009</td>\n",
       "      <td>297.619995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TR.AV</td>\n",
       "      <td>249</td>\n",
       "      <td>3026</td>\n",
       "      <td>ECA</td>\n",
       "      <td>Member State</td>\n",
       "      <td>TUR</td>\n",
       "      <td>...</td>\n",
       "      <td>192.989990</td>\n",
       "      <td>436.459991</td>\n",
       "      <td>287.709991</td>\n",
       "      <td>233.379990</td>\n",
       "      <td>320.510010</td>\n",
       "      <td>395.299988</td>\n",
       "      <td>330.269989</td>\n",
       "      <td>314.760010</td>\n",
       "      <td>366.470001</td>\n",
       "      <td>327.230011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TR.AV</td>\n",
       "      <td>249</td>\n",
       "      <td>3026</td>\n",
       "      <td>ECA</td>\n",
       "      <td>Member State</td>\n",
       "      <td>TUR</td>\n",
       "      <td>...</td>\n",
       "      <td>235.299988</td>\n",
       "      <td>381.230011</td>\n",
       "      <td>322.149994</td>\n",
       "      <td>279.820007</td>\n",
       "      <td>396.690002</td>\n",
       "      <td>443.660004</td>\n",
       "      <td>392.470001</td>\n",
       "      <td>444.039978</td>\n",
       "      <td>507.309998</td>\n",
       "      <td>458.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TR.AV</td>\n",
       "      <td>249</td>\n",
       "      <td>3026</td>\n",
       "      <td>ECA</td>\n",
       "      <td>Member State</td>\n",
       "      <td>TUR</td>\n",
       "      <td>...</td>\n",
       "      <td>203.569992</td>\n",
       "      <td>285.660004</td>\n",
       "      <td>253.250000</td>\n",
       "      <td>135.119995</td>\n",
       "      <td>239.789993</td>\n",
       "      <td>283.209991</td>\n",
       "      <td>217.059998</td>\n",
       "      <td>285.390015</td>\n",
       "      <td>329.330017</td>\n",
       "      <td>186.710007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TUR</td>\n",
       "      <td>TR</td>\n",
       "      <td>TR.AV</td>\n",
       "      <td>249</td>\n",
       "      <td>3026</td>\n",
       "      <td>ECA</td>\n",
       "      <td>Member State</td>\n",
       "      <td>TUR</td>\n",
       "      <td>...</td>\n",
       "      <td>893.239990</td>\n",
       "      <td>1747.179932</td>\n",
       "      <td>1107.069946</td>\n",
       "      <td>970.840088</td>\n",
       "      <td>1150.010010</td>\n",
       "      <td>1144.910034</td>\n",
       "      <td>1221.699951</td>\n",
       "      <td>1075.979980</td>\n",
       "      <td>1422.750000</td>\n",
       "      <td>1259.769897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ISO_A3 ISO_A2 WB_A3 HASC_0 HASC_1  GAUL_0  GAUL_1 WB_REGION     WB_STATUS  \\\n",
       "0    TUR     TR   TUR     TR  TR.AV     249    3026       ECA  Member State   \n",
       "1    TUR     TR   TUR     TR  TR.AV     249    3026       ECA  Member State   \n",
       "2    TUR     TR   TUR     TR  TR.AV     249    3026       ECA  Member State   \n",
       "3    TUR     TR   TUR     TR  TR.AV     249    3026       ECA  Member State   \n",
       "4    TUR     TR   TUR     TR  TR.AV     249    3026       ECA  Member State   \n",
       "\n",
       "  SOVEREIGN  ... sum_viirs_ntl_2015 sum_viirs_ntl_2016 sum_viirs_ntl_2017  \\\n",
       "0       TUR  ...         179.200012         297.839996         257.440002   \n",
       "1       TUR  ...         192.989990         436.459991         287.709991   \n",
       "2       TUR  ...         235.299988         381.230011         322.149994   \n",
       "3       TUR  ...         203.569992         285.660004         253.250000   \n",
       "4       TUR  ...         893.239990        1747.179932        1107.069946   \n",
       "\n",
       "  sum_viirs_ntl_2018 sum_viirs_ntl_2019 sum_viirs_ntl_2020 sum_viirs_ntl_2021  \\\n",
       "0         183.550018         241.480011         285.760010         331.869995   \n",
       "1         233.379990         320.510010         395.299988         330.269989   \n",
       "2         279.820007         396.690002         443.660004         392.470001   \n",
       "3         135.119995         239.789993         283.209991         217.059998   \n",
       "4         970.840088        1150.010010        1144.910034        1221.699951   \n",
       "\n",
       "  sum_viirs_ntl_2022 sum_viirs_ntl_2023 sum_viirs_ntl_2024  \n",
       "0         288.160034         346.290009         297.619995  \n",
       "1         314.760010         366.470001         327.230011  \n",
       "2         444.039978         507.309998         458.549988  \n",
       "3         285.390015         329.330017         186.710007  \n",
       "4        1075.979980        1422.750000        1259.769897  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the nighttime lights values for the AOI\n",
    "properties = client.get_properties(\"nighttime_lights\")\n",
    "sel_fields = list(properties['name'].values[:-1])\n",
    "df = client.get_summary(\n",
    "    gdf=adm1_boundaries,                     # Area of Interest\n",
    "    spatial_join_method=\"centroid\",         # Spatial join method (between h3 cells and each feature)\n",
    "    fields=sel_fields,                      # Fields from Space2Stats to query\n",
    "    geometry=\"polygon\"                      # Whether to return the geometry of the hexagons\n",
    ")\n",
    "\n",
    "df[\"geometry\"] = df[\"geometry\"].apply(lambda geom: from_geojson(geom))\n",
    "\n",
    "# Convert dataframe to GeoDataFrame\n",
    "tur_s2s = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "tur_s2s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h3ronpy.pandas.vector import geodataframe_to_cells, cells_dataframe_to_geodataframe\n",
    "from h3ronpy import ContainmentMode\n",
    "\n",
    "def get_bounds(in_shp, gID, h3_lvl=6):\n",
    "    \"\"\" Generate a geodataframe for the supplied in_shp with the H3 cells and % overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_shp : geopandas.GeoDataFrame\n",
    "        The input shapely polygon as a geopandas dataframe\n",
    "    gID : string\n",
    "        The column name to use for the ID in the output\n",
    "    h3_lvl : int\n",
    "        The H3 level to use for the hexagons, default is 6\n",
    "    \"\"\"\n",
    "    # extract the H3 cells\n",
    "    cols_to_keep = [gID, 'cell', 'overlap']\n",
    "    cell_ax = cells_dataframe_to_geodataframe(geodataframe_to_cells(in_shp, 6, ContainmentMode.IntersectsBoundary))\n",
    "    cell_ax['cell'] = cell_ax['cell'].apply(lambda x: hex(x)[2:])    \n",
    "    # Identify contained and overlapping hexes with the admin bounds\n",
    "    contained_h3 = cell_ax.sjoin(in_shp, predicate='within')\n",
    "    missed_h3 = cell_ax[~cell_ax['cell'].isin(contained_h3['cell'])]\n",
    "    # calculate h3x overlap with feature\n",
    "    shp_area = in_shp.union_all()\n",
    "    cell_ax['overlap'] = 0.0\n",
    "    cell_ax.loc[contained_h3.index, 'overlap'] = 1.0\n",
    "    cell_ax.loc[missed_h3.index, 'overlap'] = cell_ax.loc[missed_h3.index,'geometry'].apply(lambda x: x.intersection(shp_area).area/x.area)\\\n",
    "    \n",
    "    return cell_ax.loc[:, cols_to_keep].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mgeodataframe_to_cells\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgdf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeodataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mresolution\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcontainment_mode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mContainmentMode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mContainmentMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContainsCentroid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcompact\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcell_column_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cell'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Convert a `GeoDataFrame` to H3 cells while exploding all other columns according to the number of cells derived\n",
      "from the rows geometry.\n",
      "\n",
      "The conversion of GeoDataFrames is parallelized using the available CPUs.\n",
      "\n",
      "The duplication of all non-cell columns leads to increased memory requirements. Depending on the use-case\n",
      "some of the more low-level conversion functions should be preferred.\n",
      "\n",
      ":param gdf:\n",
      ":param resolution: H3 resolution\n",
      ":param containment_mode: Containment mode used to decide if a cell is contained in a polygon or not.\n",
      "        See the ContainmentMode class.\n",
      ":param compact: Compact the returned cells by replacing cells with their parent cells when all children\n",
      "        of that cell are part of the set.\n",
      ":param cell_column_name:\n",
      ":return:\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\wbg\\anaconda3\\envs\\s2s_ingest\\lib\\site-packages\\h3ronpy\\pandas\\vector.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "geodataframe_to_cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiPolygon' object has no attribute 'geometry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lbl, curD \u001b[38;5;129;01min\u001b[39;00m tur_s2s\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADM1CD_c\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m     cur_admin_boundary \u001b[38;5;241m=\u001b[39m adm1_boundaries[adm1_boundaries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADM1CD_c\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m lbl]\n\u001b[1;32m----> 4\u001b[0m     hex_overlap \u001b[38;5;241m=\u001b[39m \u001b[43mget_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_admin_boundary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mADM1CD_c\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mget_bounds\u001b[1;34m(in_shp, gID, h3_lvl)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# extract the H3 cells\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cols_to_keep \u001b[38;5;241m=\u001b[39m [gID, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverlap\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m cell_ax \u001b[38;5;241m=\u001b[39m cells_dataframe_to_geodataframe(\u001b[43mgeodataframe_to_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_shp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mContainmentMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntersectsBoundary\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m cell_ax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cell_ax[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mhex\u001b[39m(x)[\u001b[38;5;241m2\u001b[39m:])    \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Identify contained and overlapping hexes with the admin bounds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\wbg\\Anaconda3\\envs\\s2s_ingest\\lib\\site-packages\\h3ronpy\\pandas\\vector.py:100\u001b[0m, in \u001b[0;36mgeodataframe_to_cells\u001b[1;34m(gdf, resolution, containment_mode, compact, cell_column_name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeodataframe_to_cells\u001b[39m(\n\u001b[0;32m     75\u001b[0m     gdf: gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame,\n\u001b[0;32m     76\u001b[0m     resolution: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     cell_column_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_CELL_COLUMN_NAME,\n\u001b[0;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    Convert a `GeoDataFrame` to H3 cells while exploding all other columns according to the number of cells derived\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    from the rows geometry.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     cells \u001b[38;5;241m=\u001b[39m _hv\u001b[38;5;241m.\u001b[39mwkb_to_cells(\n\u001b[1;32m--> 100\u001b[0m         \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241m.\u001b[39mto_wkb(),\n\u001b[0;32m    101\u001b[0m         resolution,\n\u001b[0;32m    102\u001b[0m         containment_mode\u001b[38;5;241m=\u001b[39mcontainment_mode,\n\u001b[0;32m    103\u001b[0m         compact\u001b[38;5;241m=\u001b[39mcompact,\n\u001b[0;32m    104\u001b[0m         flatten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(pd\u001b[38;5;241m.\u001b[39mDataFrame(gdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mgdf\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mname)))\u001b[38;5;241m.\u001b[39mappend_column(\n\u001b[0;32m    107\u001b[0m         cell_column_name, cells\n\u001b[0;32m    108\u001b[0m     )\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _explode_table_include_null(table, cell_column_name)\u001b[38;5;241m.\u001b[39mto_pandas()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MultiPolygon' object has no attribute 'geometry'"
     ]
    }
   ],
   "source": [
    "# Calculate the total nighttime lights for each administrative area, based on overlap with the administrative boundaries\n",
    "for lbl, curD in tur_s2s.groupby(\"ADM1CD_c\"):\n",
    "    cur_admin_boundary = adm1_boundaries[adm1_boundaries[\"ADM1CD_c\"] == lbl]\n",
    "    hex_overlap = get_bounds(cur_admin_boundary.union_all(), \"ADM1CD_c\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The population values are not correct in S2S, update with specific input pop raster\n",
    "res = rMisc.zonalStats(tur_s2s, pop_file, minVal=0, return_df=True)\n",
    "tur_s2s['sum_pop_2020'] = res['SUM']\n",
    "sel_fields = sel_fields + ['sum_pop_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "tur_s2s.plot(ax=ax, column=\"sum_viirs_ntl_2012\", \n",
    "         legend=True, cmap=\"Reds\", alpha=0.75, \n",
    "         scheme=\"naturalbreaks\", k=5, \n",
    "         legend_kwds=dict(title='NTL 2012', fmt=\"{:,.0f}\"),\n",
    "         linewidth=0)\n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldPhysical, verify=False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize nighttime lights and population within 15km of various railways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KML/KMZ files from the source are garbage. In order to use the KML files, we need to read them and extract the layers oddly\n",
    "from lxml import etree\n",
    "\n",
    "def list_kml_layers(filename, method=1):\n",
    "    tree = etree.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    namespaces = {'kml': 'http://www.opengis.net/kml/2.2'}\n",
    "\n",
    "    if method == 1:\n",
    "        \n",
    "        layers = root.findall(\".//{kml}Document\", namespaces)\n",
    "        layer_names = []\n",
    "        for layer in layers:\n",
    "            name_element = layer.find('{kml}name', namespaces)\n",
    "            if name_element is not None:\n",
    "                layer_names.append(name_element.text)\n",
    "            else:\n",
    "                layer_names.append(\"Unnamed Layer\")\n",
    "        return layer_names\n",
    "    else:\n",
    "        layers = []\n",
    "        for element in root.iter():\n",
    "            if element.tag.endswith('Document') or element.tag.endswith('Folder'):\n",
    "                name_element = element.find('kml:name', namespaces)\n",
    "                layer_name = name_element.text if name_element is not None else \"Unnamed Layer\"\n",
    "                layers.append(layer_name)\n",
    "        return layers\n",
    "\n",
    "def gpd_read_all_layers(filename):\n",
    "    layers = list_kml_layers(filename, method=1)\n",
    "    good_layers = []\n",
    "    for cur_layer in layers:\n",
    "        try:\n",
    "            curD = gpd.read_file(filename, driver=\"KML\", layer=cur_layer)\n",
    "            print(f\"Processing layer: {cur_layer} - {curD.union_all().geom_type}\")\n",
    "            if curD.union_all().geom_type in [\"LineString\", \"MultiLineString\"]:\n",
    "                curD['Label'] = cur_layer\n",
    "                good_layers.append(curD)\n",
    "        except:\n",
    "            print(f\"Layer {cur_layer} not found or could not be read.\")\n",
    "    return pd.concat(good_layers, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_projected_file = os.path.join(railway_folder, \"projected_railways.gpkg\")\n",
    "updated_construction_file = os.path.join(railway_folder, \"under_construction_railways.gpkg\")\n",
    "if not os.path.exists(updated_projected_file):\n",
    "    projected_rail_gpd = gpd_read_all_layers(projected_railways)\n",
    "    projected_rail_gpd.to_file(updated_projected_file, driver=\"GPKG\")\n",
    "else:\n",
    "    projected_rail_gpd = gpd.read_file(updated_projected_file)\n",
    "if not os.path.exists(updated_construction_file):\n",
    "    under_construction_rail_gpd = gpd.read_file(under_construction_railways, driver=\"KML\")    \n",
    "else:\n",
    "    under_construction_rail_gpd = gpd.read_file(updated_construction_file)\n",
    "\n",
    "existing_rail_gpd = gpd.read_file(existing_railways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert projected and under construction to single row GeoDataFrames\n",
    "projected_rail_gpd = projected_rail_gpd.dissolve().reset_index(drop=True)\n",
    "under_construction_rail_gpd = under_construction_rail_gpd.dissolve().explode().reset_index(drop=True)\n",
    "under_construction_rail_gpd.to_file(updated_construction_file, driver=\"GPKG\")\n",
    "\n",
    "# The under_construction railways are present in the projected railways file, so we need to remove them\n",
    "projected_rail_gpd['geometry'].iloc[0] = projected_rail_gpd['geometry'].iloc[0].difference(under_construction_rail_gpd['geometry'].iloc[0])\n",
    "\n",
    "# Explode the geometries to have one row per segment\n",
    "projected_rail_gpd = projected_rail_gpd.dissolve().explode().reset_index(drop=True)\n",
    "projected_rail_gpd = projected_rail_gpd.to_crs(epsg=m_crs)\n",
    "projected_rail_gpd['length'] = projected_rail_gpd['geometry'].length\n",
    "projected_rail_gpd.sort_values(by='length')\n",
    "projected_rail_gpd = projected_rail_gpd.loc[projected_rail_gpd['length'] > 10]\n",
    "projected_rail_gpd.loc[projected_rail_gpd['length'] > 10].to_file(os.path.join(railway_folder, \"projected_railways_long.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s2s_sums(in_shape, s2s, buffer_dist=15000):\n",
    "    # read in railways and buffer\n",
    "    in_shape_buffer = in_shape.to_crs(m_crs)  \n",
    "    in_shape_buffer[\"geometry\"] = in_shape_buffer.buffer(buffer_dist)\n",
    "    all_shape = in_shape_buffer.union_all() \n",
    "\n",
    "    # Identify S2S hexagons that intersect with the buffered railways\n",
    "    s2s_cols = s2s.columns.tolist()\n",
    "    s2s = gpd.sjoin(s2s, in_shape_buffer, how=\"inner\", predicate=\"intersects\")\n",
    "    s2s = s2s.drop_duplicates(subset=[\"hex_id\"])\n",
    "    s2s = s2s.loc[:, s2s_cols]  # Keep only the original S2S columns\n",
    "    \n",
    "    # determine S2S overlap with buffered railways\n",
    "    s2s['overlap'] = s2s['geometry'].apply(lambda x: x.intersection(all_shape).area/x.area)\n",
    "    combo_h3 = s2s.sjoin(in_shape_buffer, how=\"inner\", predicate=\"intersects\")\n",
    "    combo_h3 = combo_h3.drop_duplicates(subset=[\"hex_id\"])\n",
    "    \n",
    "    #Calculate sums based on overlap\n",
    "    all_results = {}\n",
    "    for col in sel_fields:\n",
    "        cur_results = (combo_h3[col] * combo_h3['overlap']).sum()\n",
    "        all_results[col] = cur_results\n",
    "    return all_results\n",
    "\n",
    "railway_results_file = os.path.join(results_folder, \"railway_s2s_summary.csv\")\n",
    "all_rails = pd.concat([existing_rail_gpd,under_construction_rail_gpd], ignore_index=True)\n",
    "if not os.path.exists(railway_results_file):\n",
    "    if tur_s2s.crs != m_crs:\n",
    "        tur_s2s = tur_s2s.to_crs(m_crs)\n",
    "    tPrint(\"Processing railways and S2S\")\n",
    "    existing_res = get_s2s_sums(existing_rail_gpd, tur_s2s, buffer_dist=15000)    \n",
    "    tPrint(\"Completed existing railways\")\n",
    "    #projected_res = get_s2s_sums(projected_rail_gpd.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    tPrint(\"Completed projected railways\")\n",
    "    under_construction_res = get_s2s_sums(under_construction_rail_gpd.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    tPrint(\"Completed under construction railways\")\n",
    "    all_res = get_s2s_sums(all_rails.dissolve(), tur_s2s, buffer_dist=15000)\n",
    "    pd.DataFrame({\n",
    "        \"All Railways\": all_res,\n",
    "        \"Existing Railways\": existing_res,\n",
    "     #   \"Projected Railways\": projected_res,\n",
    "        \"Under Construction Railways\": under_construction_res\n",
    "    }).T.to_csv(railway_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railway_results_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize effects on muncipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_boundaries = adm1_boundaries.to_crs(m_crs)\n",
    "# run summaries on tur_s2s\n",
    "all_res = {}\n",
    "for lbl, df in tur_s2s.groupby(\"ADM1CD_c\"):\n",
    "    adm_shape = adm1_boundaries.loc[adm1_boundaries[\"ADM1CD_c\"] == lbl, \"geometry\"].values[0]\n",
    "    df['overlap'] = df['geometry'].apply(lambda x: x.intersection(adm_shape).area/x.area)\n",
    "    results = {}\n",
    "    for col in sel_fields + ['sum_pop_2020']:\n",
    "        results[col] = (df[col] * df['overlap']).sum()\n",
    "    all_res[lbl] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_res).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_summaries = pd.merge(adm1_boundaries, pd.DataFrame(all_res).T, left_on=\"ADM1CD_c\", right_index=True)\n",
    "adm_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the metropolitan areas that intersect railways\n",
    "metro_adm = adm_summaries.loc[adm_summaries['Metropolitan'] == 1]\n",
    "\n",
    "# project all layers to m_crs\n",
    "projected_rail_gpd = projected_rail_gpd.to_crs(epsg=m_crs)\n",
    "under_construction_rail_gpd = under_construction_rail_gpd.to_crs(epsg=m_crs)\n",
    "existing_rail_gpd = existing_rail_gpd.to_crs(epsg=m_crs)\n",
    "metro_adm = metro_adm.to_crs(epsg=m_crs)\n",
    "metro_adm['geometry'] = metro_adm.buffer(15000)  # Buffer the metropolitan areas\n",
    "\n",
    "# Check which metropolitan areas intersect projected railways\n",
    "metro_intersections_pr = metro_adm.sjoin(projected_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_pr = metro_intersections_pr.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "# Check with which metropolitan areas intersect under construction railways\n",
    "metro_intersections_uc = metro_adm.sjoin(under_construction_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_uc = metro_intersections_uc.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "# Check which metropolitan areas intersect existing railways\n",
    "metro_intersections_ex = metro_adm.sjoin(existing_rail_gpd, how=\"inner\", predicate=\"intersects\")\n",
    "metro_intersections_ex = metro_intersections_ex.drop_duplicates(subset=[\"ADM1CD_c\"])\n",
    "\n",
    "for cur_intersections, label in zip(\n",
    "        [metro_intersections_pr, metro_intersections_uc, metro_intersections_ex],\n",
    "        [\"Projected Railways\", \"Under Construction Railways\", \"Existing Railways\"]\n",
    "    ):\n",
    "    print(f\"\\n{label}, {cur_intersections.shape[0]}, {cur_intersections['sum_pop_2020'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ntl layers\n",
    "ntl_files = ntlMisc.generate_annual_composites(adm_boundaries, out_folder=os.path.join(base_folder, \"Data\", \"NTL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tur_s2s.to_file(os.path.join(results_folder, \"tur_s2s.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1_boundaries.to_file(admin_bounds.replace(\".gpkg\", \"_metro.gpkg\"), driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_ingest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
