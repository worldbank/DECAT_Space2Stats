{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Generated Zonal Stats\n",
    "This notebook will run through a checks to see if the generated h3 zonal stats have been calculated correctly. This will include checking at the following steps:\n",
    "\n",
    "1. H1 CSV files on S3\n",
    "2. Aggregated parquet files on S3\n",
    "3. S2S database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, io, json, sys\n",
    "import urllib3\n",
    "import boto3\n",
    "import rasterio\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium as flm\n",
    "import GOSTrocks.rasterMisc as rMisc\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from shapely.geometry import shape, box\n",
    "from geojson_pydantic import Feature, Polygon\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from pystac_client import Client\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "\n",
    "import global_zonal\n",
    "import h3_helper\n",
    "\n",
    "urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "def tPrint(s):\n",
    "    \"\"\"prints the time along with the message\"\"\"\n",
    "    print(\"%s\\t%s\" % (time.strftime(\"%H:%M:%S\"), s))\n",
    "\n",
    "s3_client = boto3.client('s3', verify=False, config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle file h0_dictionary_of_h6_geodata_frames_land.pickle: it exists True\n"
     ]
    }
   ],
   "source": [
    "# Define input variables\n",
    "base_folder = \"C:/WBG/Work/S2S/data/landcover\"\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder)\n",
    "\n",
    "bucket = \"io-10m-annual-lulc\"\n",
    "\n",
    "h3_0_list = h3_helper.generate_lvl0_lists(\n",
    "    6,\n",
    "    return_gdf=True,\n",
    "    buffer0=False,\n",
    "    read_pickle=True,\n",
    "    pickle_file=\"h0_dictionary_of_h6_geodata_frames_land.pickle\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h3_0_list.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:26:27\tProcessing 8001fffffffffff with 10516 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8001fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8003fffffffffff with 37740 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8003fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8005fffffffffff with 51141 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8005fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8007fffffffffff with 51656 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8007fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8009fffffffffff with 39466 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8009fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 800bfffffffffff with 114016 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_800bfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 800dfffffffffff with 59002 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_800dfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 800ffffffffffff with 69309 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_800ffffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8011fffffffffff with 110447 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8011fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8013fffffffffff with 111635 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8013fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8015fffffffffff with 109233 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8015fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8017fffffffffff with 30219 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8017fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8019fffffffffff with 18011 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8019fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 801bfffffffffff with 9786 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_801bfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 801dfffffffffff with 44 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_801dfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 801ffffffffffff with 99411 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_801ffffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8021fffffffffff with 120228 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8021fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8023fffffffffff with 2973 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8023fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8025fffffffffff with 118630 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8025fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8027fffffffffff with 118168 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8027fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8029fffffffffff with 46609 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8029fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 802bfffffffffff with 47178 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_802bfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 802dfffffffffff with 97535 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_802dfffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 802ffffffffffff with 19039 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_802ffffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8031fffffffffff with 68036 features\n",
      "File C:/WBG/Work/S2S/data/landcover\\landcover_8031fffffffffff_2022.parquet already exists, skipping processing\n",
      "09:26:27\tProcessing 8033fffffffffff with 5043 features\n",
      "Processing 55T_2022 with 5043 features\n",
      "Processing 54T_2022 with 5043 features\n",
      "Processing 54S_2022 with 5043 features\n",
      "Processing 54R_2022 with 5043 features\n",
      "Processing 53T_2022 with 5043 features\n",
      "Processing 53S_2022 with 5043 features\n",
      "Processing 52T_2022 with 5043 features\n",
      "Processing 52S_2022 with 5043 features\n",
      "Processing 52R_2022 with 5043 features\n",
      "Processing 51T_2022 with 5043 features\n",
      "Processing 51S_2022 with 5043 features\n",
      "Processing 51R_2022 with 5043 features\n",
      "Processing 50T_2022 with 5043 features\n",
      "Processing 50S_2022 with 5043 features\n",
      "Processing 50R_2022 with 5043 features\n",
      "Processing 49T_2022 with 5043 features\n",
      "Processing 49S_2022 with 5043 features\n",
      "Processing 49R_2022 with 5043 features\n",
      "Processing 48T_2022 with 5043 features\n",
      "Processing 48S_2022 with 5043 features\n",
      "Processing 48R_2022 with 5043 features\n",
      "Processing 47T_2022 with 5043 features\n",
      "Processing 47S_2022 with 5043 features\n",
      "Processing 47R_2022 with 5043 features\n",
      "Processing 46T_2022 with 5043 features\n",
      "Processing 46S_2022 with 5043 features\n",
      "Processing 46R_2022 with 5043 features\n",
      "Processing 45T_2022 with 5043 features\n",
      "Processing 45S_2022 with 5043 features\n",
      "Processing 45R_2022 with 5043 features\n",
      "Processing 44T_2022 with 5043 features\n",
      "Processing 44S_2022 with 5043 features\n",
      "Processing 44R_2022 with 5043 features\n",
      "Processing 43T_2022 with 5043 features\n",
      "Processing 43S_2022 with 5043 features\n",
      "Processing 43R_2022 with 5043 features\n",
      "Processing 42T_2022 with 5043 features\n",
      "Processing 42S_2022 with 5043 features\n",
      "Processing 42R_2022 with 5043 features\n",
      "Processing 41T_2022 with 5043 features\n",
      "Processing 41S_2022 with 5043 features\n",
      "Processing 41R_2022 with 5043 features\n",
      "Processing 40T_2022 with 5043 features\n",
      "Processing 40S_2022 with 5043 features\n",
      "Processing 40R_2022 with 5043 features\n",
      "Processing 39T_2022 with 5043 features\n",
      "Processing 39S_2022 with 5043 features\n",
      "Processing 39R_2022 with 5043 features\n",
      "Processing 38T_2022 with 5043 features\n",
      "Processing 38S_2022 with 5043 features\n",
      "Processing 38R_2022 with 5043 features\n",
      "Processing 37T_2022 with 5043 features\n",
      "Processing 37S_2022 with 5043 features\n",
      "Processing 37R_2022 with 5043 features\n",
      "Processing 36T_2022 with 5043 features\n",
      "Processing 36S_2022 with 5043 features\n",
      "Processing 36R_2022 with 5043 features\n",
      "Processing 35T_2022 with 5043 features\n",
      "Processing 35S_2022 with 5043 features\n",
      "Processing 35R_2022 with 5043 features\n",
      "Processing 34T_2022 with 5043 features\n",
      "Processing 34S_2022 with 5043 features\n",
      "Processing 34R_2022 with 5043 features\n",
      "Processing 33T_2022 with 5043 features\n",
      "Processing 33S_2022 with 5043 features\n",
      "Processing 33R_2022 with 5043 features\n",
      "Processing 32T_2022 with 5043 features\n",
      "Processing 32S_2022 with 5043 features\n",
      "Processing 32R_2022 with 5043 features\n",
      "Processing 31T_2022 with 5043 features\n",
      "Processing 31S_2022 with 5043 features\n",
      "Processing 31R_2022 with 5043 features\n",
      "Processing 30T_2022 with 5043 features\n",
      "Processing 30S_2022 with 5043 features\n",
      "Processing 30R_2022 with 5043 features\n",
      "Processing 29T_2022 with 5043 features\n",
      "Processing 29S_2022 with 5043 features\n",
      "Processing 29R_2022 with 5043 features\n",
      "Processing 28S_2022 with 5043 features\n",
      "Processing 28R_2022 with 5043 features\n",
      "Processing 26S_2022 with 5043 features\n",
      "Processing 25S_2022 with 5043 features\n",
      "Processing 21T_2022 with 5043 features\n",
      "Processing 20T_2022 with 5043 features\n",
      "Processing 20S_2022 with 5043 features\n",
      "Processing 19T_2022 with 5043 features\n",
      "Processing 18T_2022 with 5043 features\n",
      "Processing 18S_2022 with 5043 features\n",
      "Processing 17T_2022 with 5043 features\n",
      "Processing 17S_2022 with 5043 features\n",
      "Processing 17R_2022 with 5043 features\n",
      "Processing 16T_2022 with 5043 features\n",
      "Processing 16S_2022 with 5043 features\n",
      "Processing 16R_2022 with 5043 features\n",
      "Processing 15T_2022 with 5043 features\n",
      "Processing 15S_2022 with 5043 features\n",
      "Processing 15R_2022 with 5043 features\n",
      "Processing 14T_2022 with 5043 features\n",
      "Processing 14S_2022 with 5043 features\n",
      "Processing 14R_2022 with 5043 features\n",
      "Processing 13T_2022 with 5043 features\n",
      "Processing 13S_2022 with 5043 features\n",
      "Processing 13R_2022 with 5043 features\n",
      "Processing 12T_2022 with 5043 features\n",
      "Processing 12S_2022 with 5043 features\n",
      "Processing 12R_2022 with 5043 features\n",
      "Processing 11T_2022 with 5043 features\n",
      "Processing 11S_2022 with 5043 features\n",
      "Processing 11R_2022 with 5043 features\n",
      "Processing 10T_2022 with 5043 features\n",
      "Processing 10S_2022 with 5043 features\n",
      "Processing 55T_2021 with 5043 features\n",
      "Processing 54T_2021 with 5043 features\n",
      "Processing 54S_2021 with 5043 features\n",
      "Processing 54R_2021 with 5043 features\n",
      "Processing 53T_2021 with 5043 features\n",
      "Processing 53S_2021 with 5043 features\n",
      "Processing 52T_2021 with 5043 features\n",
      "Processing 52S_2021 with 5043 features\n",
      "Processing 52R_2021 with 5043 features\n",
      "Processing 51T_2021 with 5043 features\n",
      "Processing 51S_2021 with 5043 features\n",
      "Processing 51R_2021 with 5043 features\n",
      "Processing 50T_2021 with 5043 features\n",
      "Processing 50S_2021 with 5043 features\n",
      "Processing 50R_2021 with 5043 features\n",
      "Processing 49T_2021 with 5043 features\n",
      "Processing 49S_2021 with 5043 features\n",
      "Processing 49R_2021 with 5043 features\n",
      "Processing 48T_2021 with 5043 features\n",
      "Processing 48S_2021 with 5043 features\n",
      "Processing 48R_2021 with 5043 features\n",
      "Processing 47T_2021 with 5043 features\n",
      "Processing 47S_2021 with 5043 features\n",
      "Processing 47R_2021 with 5043 features\n",
      "Processing 46T_2021 with 5043 features\n",
      "Processing 46S_2021 with 5043 features\n",
      "Processing 46R_2021 with 5043 features\n",
      "Processing 45T_2021 with 5043 features\n",
      "Processing 45S_2021 with 5043 features\n",
      "Processing 45R_2021 with 5043 features\n",
      "Processing 44T_2021 with 5043 features\n",
      "Processing 44S_2021 with 5043 features\n",
      "Processing 44R_2021 with 5043 features\n",
      "Processing 43T_2021 with 5043 features\n",
      "Processing 43S_2021 with 5043 features\n",
      "Processing 43R_2021 with 5043 features\n",
      "Processing 42T_2021 with 5043 features\n",
      "Processing 42S_2021 with 5043 features\n",
      "Processing 42R_2021 with 5043 features\n",
      "Processing 41T_2021 with 5043 features\n",
      "Processing 41S_2021 with 5043 features\n",
      "Processing 41R_2021 with 5043 features\n",
      "Processing 40T_2021 with 5043 features\n",
      "Processing 40S_2021 with 5043 features\n",
      "Processing 40R_2021 with 5043 features\n",
      "Processing 39T_2021 with 5043 features\n",
      "Processing 39S_2021 with 5043 features\n",
      "Processing 39R_2021 with 5043 features\n",
      "Processing 38T_2021 with 5043 features\n",
      "Processing 38S_2021 with 5043 features\n",
      "Processing 38R_2021 with 5043 features\n",
      "Processing 37T_2021 with 5043 features\n",
      "Processing 37S_2021 with 5043 features\n",
      "Processing 37R_2021 with 5043 features\n",
      "Processing 36T_2021 with 5043 features\n",
      "Processing 36S_2021 with 5043 features\n",
      "Processing 36R_2021 with 5043 features\n",
      "Processing 35T_2021 with 5043 features\n",
      "Processing 35S_2021 with 5043 features\n",
      "Processing 35R_2021 with 5043 features\n",
      "Processing 34T_2021 with 5043 features\n",
      "Processing 34S_2021 with 5043 features\n",
      "Processing 34R_2021 with 5043 features\n",
      "Processing 33T_2021 with 5043 features\n",
      "Processing 33S_2021 with 5043 features\n",
      "Processing 33R_2021 with 5043 features\n",
      "Processing 32T_2021 with 5043 features\n",
      "Processing 32S_2021 with 5043 features\n",
      "Processing 32R_2021 with 5043 features\n",
      "Processing 31T_2021 with 5043 features\n",
      "Processing 31S_2021 with 5043 features\n",
      "Processing 31R_2021 with 5043 features\n",
      "Processing 30T_2021 with 5043 features\n",
      "Processing 30S_2021 with 5043 features\n",
      "Processing 30R_2021 with 5043 features\n",
      "Processing 29T_2021 with 5043 features\n",
      "Processing 29S_2021 with 5043 features\n",
      "Processing 29R_2021 with 5043 features\n",
      "Processing 28S_2021 with 5043 features\n",
      "Processing 28R_2021 with 5043 features\n",
      "Processing 26S_2021 with 5043 features\n",
      "Processing 25S_2021 with 5043 features\n",
      "Processing 21T_2021 with 5043 features\n",
      "Processing 20T_2021 with 5043 features\n",
      "Processing 20S_2021 with 5043 features\n",
      "Processing 19T_2021 with 5043 features\n",
      "Processing 18T_2021 with 5043 features\n",
      "Processing 18S_2021 with 5043 features\n",
      "Processing 17T_2021 with 5043 features\n",
      "Processing 17S_2021 with 5043 features\n",
      "Processing 17R_2021 with 5043 features\n",
      "Processing 16T_2021 with 5043 features\n",
      "Processing 16S_2021 with 5043 features\n",
      "Processing 16R_2021 with 5043 features\n",
      "Processing 15T_2021 with 5043 features\n",
      "Processing 15S_2021 with 5043 features\n",
      "Processing 15R_2021 with 5043 features\n",
      "Processing 14T_2021 with 5043 features\n",
      "Processing 14S_2021 with 5043 features\n",
      "Processing 14R_2021 with 5043 features\n",
      "Processing 13T_2021 with 5043 features\n",
      "Processing 13S_2021 with 5043 features\n",
      "Processing 13R_2021 with 5043 features\n",
      "Processing 12T_2021 with 5043 features\n",
      "Processing 12S_2021 with 5043 features\n",
      "Processing 12R_2021 with 5043 features\n",
      "Processing 11T_2021 with 5043 features\n",
      "Processing 11S_2021 with 5043 features\n",
      "Processing 11R_2021 with 5043 features\n",
      "Processing 10T_2021 with 5043 features\n",
      "Processing 10S_2021 with 5043 features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00300cbfb5f640ecb7e4669c1846df83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m                 curD \u001b[38;5;241m=\u001b[39m curD\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT             \n\u001b[0;32m     53\u001b[0m             cur_all_res\u001b[38;5;241m.\u001b[39mappend(curD)\n\u001b[1;32m---> 55\u001b[0m         final_h0_res \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_all_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         final_h0_res\u001b[38;5;241m.\u001b[39mto_parquet(out_lc_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\wbg\\Anaconda3\\envs\\s2s_ingest\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\wbg\\Anaconda3\\envs\\s2s_ingest\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\wbg\\Anaconda3\\envs\\s2s_ingest\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "for h0_lbl, h0_level in h3_0_list.items():\n",
    "    out_lc_file = os.path.join(base_folder, f\"landcover_{h0_lbl}_2022.parquet\")\n",
    "    if not os.path.exists(out_lc_file):\n",
    "        tPrint(f\"Processing {h0_lbl} with {len(h0_level)} features\")    \n",
    "        #https://pystac-client.readthedocs.io/en/latest/tutorials/pystac-client-introduction.html#API-Search\n",
    "        query = catalog.search(\n",
    "            collections=[\"io-lulc-9-class\"],\n",
    "            datetime=\"2022-01-01/2022-12-31\",\n",
    "            intersects=h0_level.geometry.union_all(),\n",
    "        )\n",
    "        query_items = list(query.item_collection())\n",
    "\n",
    "        h0_level_result = h0_level.copy()\n",
    "        all_res = []\n",
    "        for lc_feature in query_items:# tqdm(query_items, desc=\"Processing Land Cover Features\"):\n",
    "            lc_label = lc_feature.id.replace(\"-\", \"_\")\n",
    "            try:        \n",
    "                obj = s3_client.get_object(Bucket=bucket,Key=f'{lc_label}.tif')\n",
    "                process = True\n",
    "            except:\n",
    "                print(f\"Could not find file for {lc_label}, skipping\")\n",
    "                process = False\n",
    "                continue\n",
    "            if process:\n",
    "                raw_data: bytes = obj['Body'].read()\n",
    "                cur_lc = rasterio.open(io.BytesIO(raw_data))\n",
    "                if h0_level.crs != cur_lc.crs:\n",
    "                    h0_level = h0_level.to_crs(cur_lc.crs)            \n",
    "                lc_box = gpd.GeoDataFrame(pd.DataFrame([[1, box(*cur_lc.bounds)]], columns=['id', 'geometry']), crs=cur_lc.crs, geometry='geometry')\n",
    "                sel_hexes = gpd.sjoin(h0_level, lc_box, how='inner', predicate='intersects')\n",
    "                lc_res = rMisc.zonalStats(sel_hexes, cur_lc, rastType='C', unqVals=list(range(1, 13))) \n",
    "                lc_res = pd.DataFrame(lc_res, columns=[f'c_{x}' for x in range(1, 13)])       \n",
    "                lc_res['shape_id'] = sel_hexes['shape_id'].values\n",
    "                all_res.append(lc_res)\n",
    "        if len(all_res) > 0:\n",
    "            cur_h0_res = pd.concat(all_res, ignore_index=True)\n",
    "            cur_h0_res.set_index('shape_id', inplace=True)\n",
    "            cur_h0_res['total_lc_cells'] = cur_h0_res.sum(axis=1)\n",
    "            cur_h0_res = cur_h0_res.loc[cur_h0_res['total_lc_cells'] > 0]\n",
    "            cur_h0_res['hex_id'] = cur_h0_res.index.values\n",
    "\n",
    "            # If there are duplicates, sum the values for each hex_id\n",
    "            cur_all_res = []\n",
    "            for hex_id, curD in tqdm(cur_h0_res.groupby('hex_id')):\n",
    "                if len(curD) > 1:\n",
    "                    # sum the columns        \n",
    "                    curD = curD.sum()\n",
    "                    curD['hex_id'] = hex_id   \n",
    "                    curD = curD.to_frame().T             \n",
    "                cur_all_res.append(curD)\n",
    "\n",
    "            if len(cur_all_res) > 0:\n",
    "                final_h0_res = pd.concat(cur_all_res)\n",
    "                final_h0_res.to_parquet(out_lc_file, index=False)\n",
    "    else:\n",
    "        print(f\"File {out_lc_file} already exists, skipping processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>c_10</th>\n",
       "      <th>c_11</th>\n",
       "      <th>c_12</th>\n",
       "      <th>total_lc_cells</th>\n",
       "      <th>hex_id</th>\n",
       "      <th>86006a007ffffff</th>\n",
       "      <th>86006a267ffffff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>28170</td>\n",
       "      <td>0</td>\n",
       "      <td>1603</td>\n",
       "      <td>0</td>\n",
       "      <td>192436</td>\n",
       "      <td>860001047ffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>419685</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6731</td>\n",
       "      <td>238631</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>665226</td>\n",
       "      <td>860001067ffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1875</td>\n",
       "      <td>399732</td>\n",
       "      <td>0</td>\n",
       "      <td>2321</td>\n",
       "      <td>0</td>\n",
       "      <td>718830</td>\n",
       "      <td>86000106fffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>363718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1517</td>\n",
       "      <td>352563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>717798</td>\n",
       "      <td>86000114fffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>10354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>719134</td>\n",
       "      <td>86000116fffffff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_1 c_2 c_3 c_4 c_5 c_6 c_7   c_8     c_9 c_10  c_11 c_12  \\\n",
       "0  162562   0   0   0   0   0   0   101   28170    0  1603    0   \n",
       "0  419685   0   0   0   0   0   0  6731  238631    0   179    0   \n",
       "0  314902   0   0   0   0   0   0  1875  399732    0  2321    0   \n",
       "0  363718   0   0   0   0   0   0  1517  352563    0     0    0   \n",
       "0  707780   0   0   0   0   0   0  1000   10354    0     0    0   \n",
       "\n",
       "  total_lc_cells           hex_id 86006a007ffffff 86006a267ffffff  \n",
       "0         192436  860001047ffffff             NaN             NaN  \n",
       "0         665226  860001067ffffff             NaN             NaN  \n",
       "0         718830  86000106fffffff             NaN             NaN  \n",
       "0         717798  86000114fffffff             NaN             NaN  \n",
       "0         719134  86000116fffffff             NaN             NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_h0_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>c_7</th>\n",
       "      <th>c_8</th>\n",
       "      <th>c_9</th>\n",
       "      <th>c_10</th>\n",
       "      <th>c_11</th>\n",
       "      <th>c_12</th>\n",
       "      <th>total_lc_cells</th>\n",
       "      <th>hex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>28170</td>\n",
       "      <td>0</td>\n",
       "      <td>1603</td>\n",
       "      <td>0</td>\n",
       "      <td>192436</td>\n",
       "      <td>860001047ffffff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      c_1 c_2 c_3 c_4 c_5 c_6 c_7  c_8    c_9 c_10  c_11 c_12 total_lc_cells  \\\n",
       "0  162562   0   0   0   0   0   0  101  28170    0  1603    0         192436   \n",
       "\n",
       "            hex_id  \n",
       "0  860001047ffffff  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_all_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_res = rMisc.zonalStats(h0_level, cur_lc, rastType='C', unqVals=list(range(1, 10)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import odc.stac\n",
    "import xarray as xr\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#projection\n",
    "SMepsg = 3857  #https://epsg.io/3857 Geographic crs, units are m\n",
    "SMepsg_str = \"epsg:{0}\".format(SMepsg)\n",
    "\n",
    "#spatial resolution (in units of projection)\n",
    "datares = 10\n",
    "# Disable SSL verification for requests\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = ''\n",
    "\n",
    "#https://odc-stac.readthedocs.io/en/latest/_api/odc.stac.load.html\n",
    "lcxr = odc.stac.load(\n",
    "    query.item_collection(),             #load the items from our query above\n",
    "    chunks={},         #use Dask to speed loading\n",
    "    dtype='int',\n",
    "    geopolygon=h0_level.geometry.iloc[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcxr.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_h0 = h0_level.to_crs(32658)\n",
    "lcxr.rio.clip([temp_h0.geometry.values[0]], crs=32658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the h1_cells and years and see if there are any actual results\n",
    "for root, folders, files in os.walk(csv_folder):\n",
    "    for f in files:\n",
    "        if f.endswith(\".csv\"):\n",
    "            cur_path = os.path.join(root, f)\n",
    "            h1 = os.path.basename(root)\n",
    "            year = f.split(\"_\")[-1][1:5]\n",
    "            cur_d = pd.read_csv(cur_path, index_col=0)\n",
    "            max_val = cur_d.max(skipna=True)[:4].max(skipna=True)\n",
    "            if max_val > 0:\n",
    "                break\n",
    "            else:\n",
    "                print(f\"no data for {h1} {year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test zonal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_level = 6\n",
    "ghsl_folder = \"C:/WBG/Work/data/GHSL\"\n",
    "ghsl_files = [os.path.join(ghsl_folder, f) for f in os.listdir(ghsl_folder) if f.endswith(\".tif\")]\n",
    "out_folder = \"C:/WBG/Work/S2S/data/GHSL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_0_list = h3_helper.generate_lvl0_lists(h3_level, return_gdf=True, buffer0=False, \n",
    "                read_pickle=True, pickle_file=\"h0_dictionary_of_h6_geodata_frames_land.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h3_0_key, h6_list in h3_0_list.items():\n",
    "    print(f\"Processing {h3_0_key}: {len(h6_list)} hexes\")\n",
    "\n",
    "sample_h0 = '8007fffffffffff'\n",
    "inH = h3_0_list[sample_h0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(out_folder, f\"{sample_h0}_ghsl_stats.csv\")\n",
    "zonal_res = global_zonal.zonal_stats_numerical(inH, 'shape_id', ghsl_files[0], out_file, minVal=0, maxVal=100000)\n",
    "zonal_res[out_file].max(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write h3 cells and clipped GHSL data to file\n",
    "temp_folder = os.path.join(out_folder, \"temp\")\n",
    "if not os.path.exists(temp_folder):\n",
    "    os.makedirs(temp_folder)\n",
    "\n",
    "temp_h3_file = os.path.join(temp_folder, f\"{sample_h0}_h3.shp\")\n",
    "if not os.path.exists(temp_h3_file):\n",
    "    inH.to_file(temp_h3_file)\n",
    "\n",
    "temp_ghsl_file = os.path.join(temp_folder, f\"{sample_h0}_ghsl.tif\")\n",
    "if not os.path.exists(temp_ghsl_file):\n",
    "    rMisc.clipRaster(ghsl_files[0], inH, temp_ghsl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_h6_id = '86075d8b7ffffff'\n",
    "res = zonal_res[out_file]\n",
    "res.loc[res['id'] == sel_h6_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = s2s_gdf.explore(\n",
    "    column=s2s_field[0],\n",
    "    tooltip=s2s_field,\n",
    "    cmap='YlGnBu',\n",
    "    legend=True,\n",
    "    scheme='naturalbreaks',\n",
    "    legend_kwds=dict(colorbar=True, caption='Population', interval=False),\n",
    "    style_kwds=dict(weight=0, fillOpacity=0.8),\n",
    "    name='Population by Hexagon'\n",
    ")\n",
    "flm.LayerControl('topright', collapsed = False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data on dev server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../../../dev_db.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with StatsTable.connect() as stats_table:\n",
    "    fields = stats_table.fields()\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = adm_boundaries.total_bounds\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_fields = ['sum_built_area_m_2000','sum_built_area_m_2020']\n",
    "AOIModel = Feature[Polygon, Dict]\n",
    "bbox = adm_boundaries.total_bounds\n",
    "\n",
    "# ~kenya\n",
    "aoi = {\n",
    "    \"type\": \"Feature\",\n",
    "    \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "            [\n",
    "                [bbox[0], bbox[1]],\n",
    "                [bbox[0], bbox[3]],\n",
    "                [bbox[2], bbox[3]],\n",
    "                [bbox[2], bbox[1]],\n",
    "                [bbox[0], bbox[1]]\n",
    "            ]\n",
    "        ],\n",
    "    },\n",
    "    \"properties\": {\"name\": \"Updated AOI\"},\n",
    "}\n",
    "\n",
    "\n",
    "feat = AOIModel(**aoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with StatsTable.connect() as stats_table:\n",
    "    data = stats_table.summaries(\n",
    "        aoi=feat, spatial_join_method=\"centroid\", fields=sel_fields, geometry=\"point\"\n",
    "    )\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change(x):\n",
    "    try:\n",
    "        return (x['sum_built_area_m_2020'] - x['sum_built_area_m_2000'])/x['sum_built_area_m_2000']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "gdf = df.copy()\n",
    "gdf['geometry'] = gdf['geometry'].apply(lambda x: shape(json.loads(x)))\n",
    "gdf = gpd.GeoDataFrame(gdf, geometry='geometry', crs=4326)\n",
    "gdf['built_change'] = gdf.apply(lambda x: get_change(x), axis=1)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf.explore(\n",
    "    column='built_change',\n",
    "    tooltip='built_change',\n",
    "    cmap='YlGnBu',\n",
    "    legend=True,\n",
    "    scheme='quantiles',\n",
    "    legend_kwds=dict(colorbar=True, caption='Population', interval=False),\n",
    "    style_kwds=dict(weight=0, fillOpacity=0.8),\n",
    "    name='Change in built area'\n",
    ")\n",
    "flm.LayerControl('topright', collapsed = False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess s3 csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s_df['h1'] = s2s_df['hex_id'].apply(lambda x: h3.cell_to_parent(x, 0))\n",
    "for unq_h1 in s2s_df['h1'].unique():\n",
    "    s3_file = s3_csv_base.format(h1=unq_h1)\n",
    "    curD = pd.read_csv(s3_file, index_col=0)\n",
    "curD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geom(x):\n",
    "    xx = h3.cell_to_latlng(x)\n",
    "    return(Point([xx[1], xx[0]]))\n",
    "curD = curD.reset_index()\n",
    "curD = curD.merge(s2s_gdf, left_on=\"id\", right_on=\"hex_id\", how='right')\n",
    "curD = gpd.GeoDataFrame(curD, geometry='geometry', crs=4326).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = curD.explore(\n",
    "    column=s2s_field,\n",
    "    tooltip=s2s_field,\n",
    "    cmap='YlGnBu',\n",
    "    legend=True,\n",
    "    scheme='naturalbreaks',\n",
    "    legend_kwds=dict(colorbar=True, caption='Population', interval=False),\n",
    "    style_kwds=dict(weight=0, fillOpacity=0.8),\n",
    "    name='Population by Hexagon'\n",
    ")\n",
    "flm.LayerControl('topright', collapsed = False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From S3 Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_parquet_file = r\"C:\\WBG\\Work\\S2S\\ingest\\GHSL_built_area_m.parquet\"\n",
    "gdf = pd.read_parquet(s3_parquet_file)\n",
    "#gdf.replace(np.nan, 0).to_parquet(s3_parquet_file.replace(\".parquet\", \"_zero.parquet\"))\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_columns[0] = 'hex_id'\n",
    "gdf.columns = gdf_columns\n",
    "gdf.to_parquet(s3_parquet_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_s3 = gdf.merge(curD, left_on='hex_id', right_on='id', how='right')\n",
    "gdf_s3 = gpd.GeoDataFrame(gdf_s3, geometry='geometry', crs=4326)\n",
    "gdf_s3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gdf_s3.explore(\n",
    "    column=s2s_field,\n",
    "    tooltip=s2s_field,\n",
    "    cmap='YlGnBu',\n",
    "    legend=True,\n",
    "    scheme='naturalbreaks',\n",
    "    legend_kwds=dict(colorbar=True, caption='Population', interval=False),\n",
    "    style_kwds=dict(weight=0, fillOpacity=0.8),\n",
    "    name='Population by Hexagon'\n",
    ")\n",
    "flm.LayerControl('topright', collapsed = False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
