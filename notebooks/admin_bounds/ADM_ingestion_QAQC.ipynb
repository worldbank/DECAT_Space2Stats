{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA/QC for delivered administrative boundaries\n",
    "The World Bank receives regular deliveries of official administrative boundaries. This script will process and evaluate these boundaries in several ways:\n",
    "\n",
    "- Combine the two ID columns into a single, primary key  \n",
    "  a. Check to ensure no duplicates in this new, primary key\n",
    "- Combine ADM0 file with disputed areas shapefile \n",
    "- Create ocean mask \n",
    "- Within higher heirarchical level, evaluate name duplication\n",
    "- Perform topological checks  \n",
    "  a. Ensure no overlaps  \n",
    "  b. Ensure no gaps  \n",
    "  c. Ensure all admin1 and admin2 shapes are fully contained within their heirarchical parents\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Point, Polygon, box, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_folder = r'C:\\WBG\\Work\\data\\ADMIN\\NEW_WB_BOUNDS'\n",
    "out_folder = r'C:\\WBG\\Work\\data\\ADMIN\\QAQC'\n",
    "better_formats_folder = r'C:\\WBG\\Work\\data\\ADMIN\\BETTER_FORMATS'\n",
    "if not os.path.exists(better_formats_folder):\n",
    "    os.makedirs(better_formats_folder)\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "admin0_file = os.path.join(admin_folder, 'WB_GAD_ADM0.shp')\n",
    "adm0_disputes_file = os.path.join(admin_folder, 'WB_GAD_NDLSA.shp')\n",
    "admin1_file = os.path.join(admin_folder, 'WB_GAD_ADM1.shp')\n",
    "admin2_file = os.path.join(admin_folder, 'WB_GAD_ADM2.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back to better formats folder as geopackage if they don't already exist\n",
    "def open_and_write_to_better_formats(filename, out_folder):\n",
    "    \"\"\"Open a GeoDataFrame and write it to a better format (GPKG) if it doesn't already exist.\"\"\"\n",
    "    out_file = os.path.join(better_formats_folder, os.path.basename(filename).replace(\".shp\", \".gpkg\"))\n",
    "    if not os.path.exists(out_file):\n",
    "        gdf = gpd.read_file(filename)\n",
    "        gdf.to_file(out_file, driver='GPKG')\n",
    "        print(f'Wrote {out_file}')\n",
    "    else:\n",
    "        gdf = gpd.read_file(out_file)\n",
    "        print(f'{out_file} already exists, skipping write.')\n",
    "    return gdf\n",
    "\n",
    "adm0 = gpd.read_file(admin0_file)\n",
    "adm0_disputes = gpd.read_file(adm0_disputes_file)\n",
    "adm1 = gpd.read_file(admin1_file)\n",
    "adm2 = gpd.read_file(admin2_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_id_columns(in_df, col_defs, drop_orig=False):\n",
    "    \"\"\" Combine the two columns \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df : _type_\n",
    "        _description_\n",
    "    col_defs : list\n",
    "        List of the column sets required to create official admin primary key\n",
    "        [['P_CODE_1', 'P_CODE_1_t'], ['ADM1CD', 'ADM1CD_t]']\n",
    "    drop_orig : bool, optional\n",
    "        Whether to drop the original columns after merging, by default False\n",
    "    \"\"\"\n",
    "    for col_def in col_defs:\n",
    "        out_col = col_def[-1][:-1] + \"c\"\n",
    "        in_df[out_col] = in_df[col_def[0]]\n",
    "        in_df.fillna({out_col: in_df[col_def[1]]}, inplace=True)\n",
    "        if drop_orig:\n",
    "            in_df.drop(columns=col_def, inplace=True)\n",
    "\n",
    "    return in_df\n",
    "\n",
    "merged_adm1 = merge_id_columns(adm1, [['P_CODE_1', 'P_CODE_1_t'], ['ADM1CD', 'ADM1CD_t']])\n",
    "merged_adm2 = merge_id_columns(adm2, [['P_CODE_1', 'P_CODE_1_t'], ['P_CODE_2', 'P_CODE_2_t'], ['ADM1CD', 'ADM1CD_t'], ['ADM2CD', 'ADM2CD_t']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADM1CD_c duplicates: 0\n",
      "ADM2CD_c duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(in_df, id_col, out_file):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df : _type_\n",
    "        _description_\n",
    "    id_col : _type_\n",
    "        _description_\n",
    "    out_file : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    adm1_dups = in_df[id_col].duplicated().sum()\n",
    "    print(f\"{id_col} duplicates: {adm1_dups}\")\n",
    "    if adm1_dups > 0:\n",
    "        # write duplicated records in adm1 and adm2 to QA/QC folder\n",
    "        adm1_dups = in_df[merged_adm1.duplicated(subset=[id_col], keep=False)]\n",
    "        adm1_dups.to_file(out_file, driver='GPKG')\n",
    "\n",
    "# Check for duplicates in ADM1\n",
    "test_col = 'ADM1CD_c'\n",
    "check_duplicates(merged_adm1, test_col, os.path.join(out_folder, f'adm1_duplicates_{test_col}.gpkg'))\n",
    "\n",
    "# Check for duplicates in ADM2\n",
    "test_col = 'ADM2CD_c'\n",
    "check_duplicates(merged_adm2, test_col, os.path.join(out_folder, f'adm2_duplicates_{test_col}.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ADM0 with disputed territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in adm0_disputes.columns:\n",
    "    if not col in adm0.columns:\n",
    "        adm0_disputes.drop(columns=[col], inplace=True)\n",
    "adm0_disputes.head()\n",
    "adm0_complete = pd.concat([adm0, adm0_disputes], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a global ocean mask from the admin divisions\n",
    "ocean_polygon = box(-180, -90, 180, 90)  # Global bounding box for ocean\n",
    "#clip ocean polygon to adm0 boundaries\n",
    "ocean_polygon = ocean_polygon.difference(adm0_complete.union_all())  # Remove land areas\n",
    "ocean_mask = gpd.GeoDataFrame([[1, ocean_polygon]], columns=['id', 'geometry'], crs=4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate name duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_duplicate_names(in_df, name_col, parent_col, log_file):\n",
    "    \"\"\" Group features by a parent_col and check if any names are duplicated\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_df : _geopandas.GeoDataFrame_\n",
    "        administrative bounds of interest\n",
    "    name_col : str\n",
    "        column containing names to check for duplicates\n",
    "    parent_col : str\n",
    "        column containing parent administrative unit code to group by\n",
    "    log_file : str\n",
    "        path to the log file where duplicates will be recorded\n",
    "    \"\"\"\n",
    "    original_stdout = sys.stdout\n",
    "    with open(log_file, 'w') as log_fh:\n",
    "        log_fh.write(f\"Checking for duplicate names in {name_col} grouped by {parent_col}\\n\")\n",
    "        log_fh.write(\"--------------------------------------------------\\n\")\n",
    "        sys.stdout = log_fh\n",
    "        for label, group in in_df.groupby(parent_col):\n",
    "            if group[name_col].duplicated().any():\n",
    "                print(f\"Duplicates found in {label} for {name_col}:\")                \n",
    "                print(group[group[name_col].duplicated(keep=False)][[name_col, parent_col]])                \n",
    "            else:\n",
    "                #print(f\"No duplicates found in {label} for {name_col}.\")\n",
    "                pass                \n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "# Evaluate duplicate names in adm1 and adm2\n",
    "evaluate_duplicate_names(merged_adm1, 'NAM_1', 'ISO_A3', os.path.join(out_folder, \"ADM1_name_duplicates.log\"))\n",
    "# Evaluate duplicate names in adm1 and adm2\n",
    "evaluate_duplicate_names(merged_adm2, 'NAM_2', 'ADM1CD_c', os.path.join(out_folder, \"ADM2_name_duplicates.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsj = gpd.sjoin(adm0, adm0, how=\"inner\", predicate=\"overlaps\", lsuffix=\"left\", rsuffix=\"right\")\\nsj = sj[sj.index != sj.index_right]\\n\\nsj[\\'intersection_geom\\'] = sj[\\'geometry_left\\'].intersection(sj[\\'geometry_right\\'])\\nsj[\\'intersection_area\\'] = sj[\\'intersection_geom\\'].area\\nsj\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sjoin to identify overlapping polygons\n",
    "'''\n",
    "sj = gpd.sjoin(adm0, adm0, how=\"inner\", predicate=\"overlaps\", lsuffix=\"left\", rsuffix=\"right\")\n",
    "sj = sj[sj.index != sj.index_right]\n",
    "\n",
    "sj['intersection_geom'] = sj['geometry_left'].intersection(sj['geometry_right'])\n",
    "sj['intersection_area'] = sj['intersection_geom'].area\n",
    "sj\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate datasets into primary and secondary tables\n",
    "\n",
    "The delivered files contain several columns that are temporary or reference external sources. This section will separate the superfluous columns into a secondary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_out_folder = os.path.join(better_formats_folder, 'simplified_output')\n",
    "if not os.path.exists(simplified_out_folder):\n",
    "    os.makedirs(simplified_out_folder)\n",
    "adm1_primary = 'ADM1CD_c'\n",
    "adm1_simple_cols = ['ISO_A3','ISO_A2','WB_A3','WB_REGION','WB_STATUS','NAM_0','NAM_1','ADM1CD_c','GEOM_SRCE', 'geometry']\n",
    "adm1_bad_cols = [adm1_primary] + [x for x in merged_adm1.columns if x not in adm1_simple_cols]\n",
    "\n",
    "adm2_primary = 'ADM2CD_c'\n",
    "adm2_simple_cols = adm1_simple_cols + ['NAM_2','ADM2CD_c']\n",
    "adm2_bad_cols = [adm2_primary] + [x for x in merged_adm2.columns if x not in adm2_simple_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out final versions in several data formats\n",
    "\n",
    "write out:\n",
    "- adm0 base\n",
    "- adm0 NDLSA\n",
    "- adm0 complete\n",
    "- adm1 simple\n",
    "- adm1 supplemntal columns\n",
    "- adm2 simple \n",
    "- adm2 supplemental columns\n",
    "\n",
    "in these formats\n",
    "- geopackage\n",
    "- geojson\n",
    "- shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_folder = os.path.join(admin_folder, 'FOR_PUBLICATION')\n",
    "if not os.path.exists(final_folder):\n",
    "    os.makedirs(final_folder)\n",
    "for file_format in ['gpkg', 'shp', 'geojson']:\n",
    "    temp_folder = os.path.join(final_folder, file_format)\n",
    "    if not os.path.exists(temp_folder):\n",
    "        os.makedirs(temp_folder)\n",
    "\n",
    "for file_def in [\n",
    "    (ocean_mask, 'WB_GAD_ocean_mask'),\n",
    "    (adm0_complete, 'WB_GAD_ADM0_complete'),\n",
    "    (adm0, 'WB_GAD_ADM0'),\n",
    "    (adm0_disputes, 'WB_GAD_ADM0_NDLSA'),\n",
    "    (merged_adm1.loc[:, adm1_simple_cols], 'WB_GAD_ADM1'),\n",
    "    (merged_adm2.loc[:, adm2_simple_cols], 'WB_GAD_ADM2'),    \n",
    "    ]:\n",
    "    gdf, filename = file_def\n",
    "    # write geopackage to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"gpkg\", f\"{filename}.gpkg\"), driver='GPKG')\n",
    "    # write shapefile to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"shp\", f\"{filename}.shp\"), driver='ESRI Shapefile')\n",
    "    # Write geojson to file\n",
    "    gdf.to_file(os.path.join(final_folder, \"geojson\", f\"{filename}.geojson\"), driver='GeoJSON')\n",
    "\n",
    "pd.DataFrame(merged_adm1.loc[:, adm1_bad_cols]).to_csv(os.path.join(final_folder, 'WB_GAD_adm1_additional_columns.csv'), index=False)\n",
    "pd.DataFrame(merged_adm2.loc[:, adm2_bad_cols]).to_csv(os.path.join(final_folder, 'WB_GAD_adm2_additional_columns.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_ingest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
